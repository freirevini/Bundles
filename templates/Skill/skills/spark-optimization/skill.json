{
  "schema_version": "1.0",
  "id": "spark-optimization",
  "title": "Apache Spark Optimization",
  "description": "Optimize Apache Spark jobs with partitioning, caching, shuffle optimization, and memory tuning. Use when improving Spark performance, debugging slow jobs, or scaling data processing pipelines.",
  "version": "1.0.0",
  "entrypoint": "SKILL.md",
  "tags": [
    "python",
    "sql"
  ],
  "tools": [],
  "priority": "normal",
  "compatibility": [
    "antigravity",
    "gemini-code-assist",
    "cursor",
    "claude-code"
  ],
  "author": "freirevini"
}
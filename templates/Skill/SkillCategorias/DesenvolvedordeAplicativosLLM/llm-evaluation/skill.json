{
  "schema_version": "1.0",
  "id": "llm-evaluation",
  "title": "LLM Evaluation",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
  "version": "1.0.0",
  "entrypoint": "SKILL.md",
  "tags": [
    "llm",
    "testing",
    "ai",
    "rag",
    "python",
    "api"
  ],
  "tools": [],
  "priority": "normal",
  "compatibility": [
    "antigravity",
    "gemini-code-assist",
    "cursor",
    "claude-code"
  ],
  "author": "freirevini"
}